import { GoogleGenerativeAI } from '@google/generative-ai';

import { aiCache } from './aiCache';
import { processVoiceToText } from './geminiService';

const ai = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');

export interface VoiceTranscriptionResult {
  transcription: string;
  confidence: number;
  processedText: string;
  detectedLanguage: string;
  duration: number;
  segments: TranscriptionSegment[];
  medicalTermsDetected: MedicalTerm[];
}

export interface TranscriptionSegment {
  text: string;
  startTime: number;
  endTime: number;
  confidence: number;
  speaker?: string;
}

export interface MedicalTerm {
  term: string;
  confidence: number;
  category: 'anatomy' | 'symptom' | 'medication' | 'procedure' | 'measurement';
  normalizedForm: string;
  position: { start: number; end: number };
}

export interface VoiceRecordingSession {
  id: string;
  patientId?: string;
  contextType: 'assessment' | 'progress' | 'prescription' | 'general';
  startTime: string;
  endTime?: string;
  recordings: VoiceRecording[];
  finalTranscription?: string;
  status: 'recording' | 'processing' | 'completed' | 'error';
}

export interface VoiceRecording {
  id: string;
  audioBlob: Blob;
  duration: number;
  timestamp: string;
  transcription?: VoiceTranscriptionResult;
}

/**
 * Simulates voice-to-text conversion with Web Speech API integration.
 * In a real implementation, this would integrate with browser's SpeechRecognition API.
 * @param audioBlob Audio blob to transcribe
 * @param options Transcription options
 * @returns Transcription result
 */
export async function transcribeAudio(
  audioBlob: Blob,
  options: {
    contextType: 'assessment' | 'progress' | 'prescription' | 'general';
    language?: string;
    medicalMode?: boolean;
  },
  userId = 'anonymous'
): Promise<VoiceTranscriptionResult> {
  // Simula a transcri√ß√£o de √°udio para texto
  // Em uma implementa√ß√£o real, isso integraria com APIs de speech-to-text
  console.log(`üéôÔ∏è Simulating audio transcription (contextType: ${options.contextType})`);
  
  try {
    // Simula√ß√£o de transcri√ß√£o - em produ√ß√£o, usaria uma API real como Google Speech-to-Text
    const simulatedTranscription = await simulateAudioTranscription(audioBlob, options);
    
    // Processa o texto transcrito com IA para melhorar a qualidade
    const processedText = await processVoiceToText(
      simulatedTranscription.transcription,
      options.contextType,
      userId
    );

    const result: VoiceTranscriptionResult = {
      transcription: simulatedTranscription.transcription,
      confidence: simulatedTranscription.confidence,
      processedText: processedText,
      detectedLanguage: options.language || 'pt-BR',
      duration: simulatedTranscription.duration,
      segments: simulatedTranscription.segments,
      medicalTermsDetected: await detectMedicalTerms(simulatedTranscription.transcription, userId)
    };

    return result;
  } catch (error) {
    console.error('Error transcribing audio:', error);
    throw new Error('Falha na transcri√ß√£o do √°udio.');
  }
}

/**
 * Starts a voice recording session for clinical documentation.
 * @param contextType Type of clinical context
 * @param patientId Optional patient ID for context
 * @returns Recording session object
 */
export function startVoiceRecordingSession(
  contextType: VoiceRecordingSession['contextType'],
  patientId?: string
): VoiceRecordingSession {
  const session: VoiceRecordingSession = {
    id: `voice_session_${Date.now()}`,
    patientId,
    contextType,
    startTime: new Date().toISOString(),
    recordings: [],
    status: 'recording'
  };

  console.log(`üéôÔ∏è Started voice recording session: ${session.id}`);
  return session;
}

/**
 * Adds a voice recording to an existing session.
 * @param sessionId Session ID
 * @param audioBlob Audio data
 * @param session Current session object
 * @returns Updated session
 */
export async function addVoiceRecording(
  sessionId: string,
  audioBlob: Blob,
  session: VoiceRecordingSession,
  userId = 'anonymous'
): Promise<VoiceRecordingSession> {
  console.log(`üéôÔ∏è Adding voice recording to session: ${sessionId}`);
  
  try {
    const recording: VoiceRecording = {
      id: `recording_${Date.now()}`,
      audioBlob,
      duration: await getAudioDuration(audioBlob),
      timestamp: new Date().toISOString()
    };

    // Transcreve o √°udio automaticamente
    try {
      recording.transcription = await transcribeAudio(
        audioBlob,
        {
          contextType: session.contextType,
          language: 'pt-BR',
          medicalMode: true
        },
        userId
      );
    } catch (transcriptionError) {
      console.warn('Transcription failed for recording:', transcriptionError);
    }

    const updatedSession = {
      ...session,
      recordings: [...session.recordings, recording]
    };

    return updatedSession;
  } catch (error) {
    console.error('Error adding voice recording:', error);
    throw new Error('Falha ao adicionar grava√ß√£o de voz.');
  }
}

/**
 * Completes a voice recording session and generates final transcription.
 * @param session Recording session to complete
 * @returns Completed session with final transcription
 */
export async function completeVoiceRecordingSession(
  session: VoiceRecordingSession,
  userId = 'anonymous'
): Promise<VoiceRecordingSession> {
  console.log(`üéôÔ∏è Completing voice recording session: ${session.id}`);
  
  try {
    session.status = 'processing';
    
    // Combina todas as transcri√ß√µes em um texto final
    const allTranscriptions = session.recordings
      .map(r => r.transcription?.processedText || r.transcription?.transcription || '')
      .filter(text => text.length > 0)
      .join(' ');

    if (allTranscriptions.length > 0) {
      // Refina o texto final com IA
      const finalTranscription = await refineFinalTranscription(
        allTranscriptions,
        session.contextType,
        userId
      );

      const completedSession: VoiceRecordingSession = {
        ...session,
        endTime: new Date().toISOString(),
        finalTranscription,
        status: 'completed'
      };

      return completedSession;
    } else {
      throw new Error('Nenhuma transcri√ß√£o v√°lida encontrada.');
    }
  } catch (error) {
    console.error('Error completing voice recording session:', error);
    return {
      ...session,
      status: 'error',
      endTime: new Date().toISOString()
    };
  }
}

/**
 * Refines the final transcription text for clinical documentation.
 * @param combinedText Combined transcription text
 * @param contextType Clinical context
 * @returns Refined transcription
 */
async function refineFinalTranscription(
  combinedText: string,
  contextType: VoiceRecordingSession['contextType'],
  userId: string
): Promise<string> {
  const cacheKey = `refine_transcription_${contextType}_${combinedText.slice(0, 50)}`;
  
  try {
    const cached = aiCache.get(cacheKey, combinedText, userId);
    if (cached) {
      console.log('‚ú® Retornando refinamento de transcri√ß√£o do cache (economia ~$0.03)');
      return cached;
    }
  } catch (error) {
    console.warn('Cache error:', error);
  }

  console.log(`ü§ñ Refining final transcription (custo ~$0.03): ${contextType}`);
  
  try {
    const contextInstructions = {
      assessment: 'Organize como uma avalia√ß√£o fisioterap√™utica estruturada, com anamnese, exame f√≠sico e impress√µes cl√≠nicas.',
      progress: 'Estruture como uma nota de evolu√ß√£o cronol√≥gica, destacando mudan√ßas no quadro cl√≠nico.',
      prescription: 'Formate como instru√ß√µes claras de exerc√≠cios e orienta√ß√µes terap√™uticas.',
      general: 'Organize e melhore a clareza do texto mantendo todas as informa√ß√µes cl√≠nicas relevantes.'
    };

    const systemInstruction = `Voc√™ √© um especialista em refinamento de transcri√ß√µes cl√≠nicas.
    Melhore o texto transcrito de voz para escrita, realizando:
    
    1. Corre√ß√£o de erros de transcri√ß√£o
    2. Melhoria da estrutura e fluxo do texto
    3. Padroniza√ß√£o da terminologia m√©dica
    4. Organiza√ß√£o l√≥gica das informa√ß√µes
    5. Remo√ß√£o de hesita√ß√µes e repeti√ß√µes desnecess√°rias
    
    Contexto espec√≠fico: ${contextInstructions[contextType]}
    
    Mantenha todo o conte√∫do cl√≠nico relevante, apenas melhorando a apresenta√ß√£o.`;

    const model = ai.getGenerativeModel({ 
      model: "gemini-pro",
      systemInstruction: systemInstruction
    });

    const response = await model.generateContent(`Refine esta transcri√ß√£o cl√≠nica: "${combinedText}"`);
    const result = await response.response;
    const refinedText = result.text();

    if (!refinedText) {
      return combinedText; // Retorna texto original se falhar
    }

    aiCache.set(cacheKey, combinedText, refinedText);
    return refinedText;
  } catch (error) {
    console.error('Error refining final transcription:', error);
    return combinedText; // Retorna texto original em caso de erro
  }
}

/**
 * Detects medical terms in transcribed text.
 * @param text Transcribed text
 * @returns Array of detected medical terms
 */
async function detectMedicalTerms(
  text: string,
  userId: string
): Promise<MedicalTerm[]> {
  const cacheKey = `medical_terms_${text.slice(0, 50)}`;
  
  try {
    const cached = aiCache.get(cacheKey, text, userId);
    if (cached) {
      console.log('üè• Retornando termos m√©dicos do cache (economia ~$0.02)');
      return JSON.parse(cached);
    }
  } catch (error) {
    console.warn('Cache error:', error);
  }

  console.log(`ü§ñ Detecting medical terms (custo ~$0.02)`);
  
  try {
    const systemInstruction = `Voc√™ √© um especialista em terminologia m√©dica fisioterap√™utica.
    Identifique e extraia termos m√©dicos do texto fornecido, classificando-os por categoria.
    
    Categorias:
    - anatomy: partes do corpo, m√∫sculos, articula√ß√µes
    - symptom: sintomas, sinais, queixas
    - medication: medicamentos, drogas
    - procedure: procedimentos, t√©cnicas, testes
    - measurement: medidas, escalas, valores
    
    Forne√ßa a forma normalizada de cada termo.`;

    const prompt = `Identifique termos m√©dicos neste texto:
    
    "${text}"
    
    Retorne JSON:
    {
      "medicalTerms": [
        {
          "term": "termo encontrado",
          "confidence": 90,
          "category": "anatomy|symptom|medication|procedure|measurement",
          "normalizedForm": "forma padronizada do termo",
          "position": {"start": 10, "end": 20}
        }
      ]
    }`;

    const model = ai.getGenerativeModel({ 
      model: "gemini-pro",
      systemInstruction: systemInstruction
    });

    const response = await model.generateContent(prompt);
    const result = await response.response;
    const termsData = JSON.parse(result.text().trim());
    
    const medicalTerms = termsData.medicalTerms || [];
    aiCache.set(cacheKey, text, JSON.stringify(medicalTerms));
    return medicalTerms;
  } catch (error) {
    console.error('Error detecting medical terms:', error);
    return [];
  }
}

/**
 * Simulates audio transcription (placeholder for real speech-to-text API).
 * @param audioBlob Audio data
 * @param options Transcription options
 * @returns Simulated transcription result
 */
async function simulateAudioTranscription(
  audioBlob: Blob,
  options: any
): Promise<{
  transcription: string;
  confidence: number;
  duration: number;
  segments: TranscriptionSegment[];
}> {
  // Simula√ß√£o - em produ√ß√£o, integraria com Google Speech-to-Text, Azure Speech, etc.
  const duration = await getAudioDuration(audioBlob);
  
  // Exemplos de transcri√ß√µes simuladas baseadas no contexto
  const simulatedTranscriptions = {
    assessment: 'Paciente relata dor no ombro direito h√° tr√™s semanas, iniciada ap√≥s atividade f√≠sica. Dor piora com movimento de eleva√ß√£o e abdu√ß√£o. N√£o h√° irradia√ß√£o. Nega trauma ou quedas.',
    progress: 'Paciente apresenta melhora significativa da dor, de oito para quatro na escala visual anal√≥gica. Amplitude de movimento aumentou em aproximadamente vinte graus. Continua com exerc√≠cios domiciliares.',
    prescription: 'Exerc√≠cios de fortalecimento dos m√∫sculos rotadores do ombro, tr√™s s√©ries de quinze repeti√ß√µes, duas vezes ao dia. Alongamento da c√°psula posterior, manter por trinta segundos.',
    general: 'Sess√£o de fisioterapia realizada com foco em mobiliza√ß√£o articular e fortalecimento muscular. Paciente tolerou bem o tratamento sem intercorr√™ncias.'
  };

  const transcription = simulatedTranscriptions[options.contextType] || simulatedTranscriptions.general;
  
  return {
    transcription,
    confidence: 0.85 + Math.random() * 0.1, // 85-95% confidence
    duration,
    segments: [{
      text: transcription,
      startTime: 0,
      endTime: duration,
      confidence: 0.9
    }]
  };
}

/**
 * Gets duration of audio blob.
 * @param audioBlob Audio data
 * @returns Duration in seconds
 */
async function getAudioDuration(audioBlob: Blob): Promise<number> {
  return new Promise((resolve) => {
    const audio = new Audio();
    audio.onloadedmetadata = () => {
      resolve(audio.duration || 0);
    };
    audio.onerror = () => {
      resolve(0); // Fallback duration
    };
    audio.src = URL.createObjectURL(audioBlob);
  });
}

/**
 * Initializes Web Speech API for real-time voice recognition.
 * This is a browser-side function that would be called from the frontend.
 */
export function initializeWebSpeechAPI(): {
  startRecognition: (callback: (text: string) => void) => void;
  stopRecognition: () => void;
  isSupported: boolean;
} {
  const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
  
  if (!SpeechRecognition) {
    return {
      startRecognition: () => console.warn('Speech recognition not supported'),
      stopRecognition: () => {},
      isSupported: false
    };
  }

  const recognition = new SpeechRecognition();
  recognition.continuous = true;
  recognition.interimResults = true;
  recognition.lang = 'pt-BR';

  return {
    startRecognition: (callback: (text: string) => void) => {
      recognition.onresult = (event: any) => {
        let finalTranscript = '';
        for (let i = event.resultIndex; i < event.results.length; i++) {
          if (event.results[i].isFinal) {
            finalTranscript += event.results[i][0].transcript;
          }
        }
        if (finalTranscript) {
          callback(finalTranscript);
        }
      };
      recognition.start();
    },
    stopRecognition: () => {
      recognition.stop();
    },
    isSupported: true
  };
}

/**
 * Converts speech to structured clinical notes with AI enhancement.
 * @param audioStream Audio stream or file
 * @param contextType Type of clinical documentation
 * @returns Enhanced clinical notes
 */
export async function speechToStructuredNotes(
  audioStream: MediaStream | Blob,
  contextType: 'assessment' | 'progress' | 'prescription' | 'general',
  userId = 'anonymous'
): Promise<{
  originalTranscription: string;
  structuredNotes: string;
  extractedData: Record<string, any>;
  confidence: number;
}> {
  console.log(`üéôÔ∏è Converting speech to structured notes: ${contextType}`);
  
  try {
    let audioBlob: Blob;
    
    if (audioStream instanceof MediaStream) {
      // Convert MediaStream to Blob (simplified - would need MediaRecorder in real implementation)
      audioBlob = new Blob([], { type: 'audio/wav' });
    } else {
      audioBlob = audioStream;
    }

    // Transcreve o √°udio
    const transcriptionResult = await transcribeAudio(
      audioBlob,
      {
        contextType,
        language: 'pt-BR',
        medicalMode: true
      },
      userId
    );

    // Extrai dados estruturados usando IA
    const extractedData = await extractStructuredDataFromSpeech(
      transcriptionResult.processedText,
      contextType,
      userId
    );

    return {
      originalTranscription: transcriptionResult.transcription,
      structuredNotes: transcriptionResult.processedText,
      extractedData,
      confidence: transcriptionResult.confidence
    };
  } catch (error) {
    console.error('Error converting speech to structured notes:', error);
    throw new Error('Falha na convers√£o de voz para notas estruturadas.');
  }
}

/**
 * Extracts structured data from processed speech text.
 * @param processedText Enhanced transcription text
 * @param contextType Clinical context
 * @returns Structured data object
 */
async function extractStructuredDataFromSpeech(
  processedText: string,
  contextType: string,
  userId: string
): Promise<Record<string, any>> {
  const cacheKey = `extract_speech_data_${contextType}_${processedText.slice(0, 50)}`;
  
  try {
    const cached = aiCache.get(cacheKey, processedText, userId);
    if (cached) {
      console.log('üìä Retornando extra√ß√£o de dados de fala do cache (economia ~$0.03)');
      return JSON.parse(cached);
    }
  } catch (error) {
    console.warn('Cache error:', error);
  }

  console.log(`ü§ñ Extracting structured data from speech (custo ~$0.03): ${contextType}`);
  
  try {
    const extractionSchemas = {
      assessment: {
        queixaPrincipal: 'string',
        historico: 'string',
        nivelDor: 'number',
        localizacaoDor: 'string',
        fatoresAliviantes: 'array',
        fatoresAgravantes: 'array',
        limitacoesFuncionais: 'array'
      },
      progress: {
        nivelDorAtual: 'number',
        nivelDorAnterior: 'number',
        melhorasObservadas: 'array',
        dificuldadesRelatadas: 'array',
        aderenciaExercicios: 'string',
        observacoes: 'string'
      },
      prescription: {
        exerciciosPrescritos: 'array',
        frequencia: 'string',
        duracao: 'string',
        precaucoes: 'array',
        orientacoes: 'string'
      },
      general: {
        pontosPrincipais: 'array',
        observacoes: 'string',
        proximosPassos: 'array'
      }
    };

    const schema = extractionSchemas[contextType as keyof typeof extractionSchemas] || extractionSchemas.general;

    const systemInstruction = `Voc√™ √© um especialista em extra√ß√£o de dados cl√≠nicos estruturados.
    Extraia informa√ß√µes espec√≠ficas do texto de transcri√ß√£o de voz e organize conforme o esquema fornecido.
    
    Seja preciso e extraia apenas informa√ß√µes explicitamente mencionadas no texto.
    Para informa√ß√µes num√©ricas (como n√≠vel de dor), extraia valores espec√≠ficos quando mencionados.`;

    const prompt = `Extraia dados estruturados deste texto de transcri√ß√£o:

"${processedText}"

Esquema de dados desejado:
${JSON.stringify(schema, null, 2)}

Retorne JSON seguindo exatamente o esquema, com valores extra√≠dos do texto ou null quando n√£o mencionado.`;

    const model = ai.getGenerativeModel({ 
      model: "gemini-pro",
      systemInstruction: systemInstruction
    });

    const response = await model.generateContent(prompt);
    const result = await response.response;
    const extractedData = JSON.parse(result.text().trim());

    aiCache.set(cacheKey, processedText, JSON.stringify(extractedData));
    return extractedData;
  } catch (error) {
    console.error('Error extracting structured data from speech:', error);
    return {};
  }
}